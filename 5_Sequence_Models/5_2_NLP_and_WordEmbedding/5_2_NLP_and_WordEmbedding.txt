-------------------------------------------------------------------------------------------------------------------------
"NATURAL LANGUAGE PROCESSING & WORD EMBEDDINGS"
Natural language processing with deep learning is an important combination. Using word vector representations and embedding layers you can train recurrent neural networks with outstanding performances in a wide variety of industries. Examples of applications are sentiment analysis, named entity recognition and machine translation.

------Introduction to Word Embeddings-----------------------------------------------------------------------------------------------------------
Word Representation
Using word embeddings
Properties of word embeddings
Embedding matrix
Embedding_Matrix.jpg
-----------------------------------------------------------------------------------------------------------
------Learning Word Embeddings: Word2vec & GloVe-----------------------------------------------------------------------------------------------------------
Learning word embeddings
	- Context-Target pair
		- Skip-gram
Word2Vec
Negative Sampling
GloVe word vectors

-----------------------------------------------------------------------------------------------------------
-----Applications using Word Embeddings--------------------------------------------------------------------------------------------------------------------
Sentiment Classification
Debiasing word embeddings

-------------------------------------------------------------------------------------------------------------------------
"NOTEBOOK: OPERATIONS ON WORD VECTORS - DEBIASING"
-------------------------------------------------------------------------------------------------------------------------

After this assignment you will be able to:
	- Load pre-trained word vectors, and measure similarity using cosine similarity
	- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __.
	- Modify word embeddings to reduce their gender bias - Debiasing

1 - Cosine similarity
2 - Word analogy task
3 - Debiasing word vectors
	3.1 - Neutralize bias for non-gender specific words
	3.2 - Equalization algorithm for gender-specific words



\1 - Cosine similarity
def cosine_similarity(u, v):
    distance = 0.0
    
    ### START CODE HERE ###
    # Compute the dot product between u and v (‚âà1 line)
    dot = u@v
    # Compute the L2 norm of u (‚âà1 line)
    norm_u = np.sqrt(np.sum(u**2))
    
    # Compute the L2 norm of v (‚âà1 line)
    norm_v = np.sqrt(np.sum(v**2))
    # Compute the cosine similarity defined by formula (1) (‚âà1 line)
    cosine_similarity = dot/(norm_u*norm_v)
    ### END CODE HERE ###
    
    return cosine_similarity

father = word_to_vec_map["father"]
mother = word_to_vec_map["mother"]
ball = word_to_vec_map["ball"]
crocodile = word_to_vec_map["crocodile"]
france = word_to_vec_map["france"]
italy = word_to_vec_map["italy"]
paris = word_to_vec_map["paris"]
rome = word_to_vec_map["rome"]

print("cosine_similarity(father, mother) = ", cosine_similarity(father, mother))
print("cosine_similarity(ball, crocodile) = ",cosine_similarity(ball, crocodile))
print("cosine_similarity(france - paris, rome - italy) = ",cosine_similarity(france - paris, rome - italy))   \
>>>
cosine_similarity(father, mother) =  0.890903844289
cosine_similarity(ball, crocodile) =  0.274392462614
cosine_similarity(france - paris, rome - italy) =  -0.675147930817



\2 - Word analogy task - eb‚àíea ‚âà ed‚àíec
In the word analogy task, we complete the sentence "a is to b as c is to __". An example is 'man is to woman as king is to queen' . In detail, we are trying to find a word d, such that the associated word vectors  ea,eb,ec,edea,eb,ec,ed  are related in the following manner:  eb‚àíea ‚âà ed‚àíec . We will measure the similarity between  eb‚àíea and  ed‚àíec  using cosine similarity.

def complete_analogy(word_a, word_b, word_c, word_to_vec_map):
    # convert words to lower case
    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()
    
    ### START CODE HERE ###
    # Get the word embeddings e_a, e_b and e_c (‚âà1-3 lines)
    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]
    ### END CODE HERE ###
    
    words = word_to_vec_map.keys()
    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number
    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output

    # loop over the whole word vector set
    for w in words:        
        # to avoid best_word being one of the input words, pass on them.
        if w in [word_a, word_b, word_c] :
            continue
        
        ### START CODE HERE ###
        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (‚âà1 line)
        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w]-e_c)
        
        # If the cosine_sim is more than the max_cosine_sim seen so far,
            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (‚âà3 lines)
        if cosine_sim > max_cosine_sim:
            max_cosine_sim = cosine_sim
            best_word = w 
        ### END CODE HERE ###
        
    return best_word

triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]
for triad in triads_to_try:
    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))
>>>
	italy -> italian :: spain -> spanish
	india -> delhi :: japan -> tokyo
	man -> woman :: boy -> girl
	small -> smaller :: large -> larger


"REMEMBER"
	- Cosine similarity a good way to compare similarity between pairs of word vectors. (Though L2 distance works too.)
	- For NLP applications, using a pre-trained set of word vectors from the internet is often a good way to get started.


\3 - Debiasing word vectors
	\3.1 - Neutralize bias for non-gender specific words
	\3.2 - Equalization algorithm for gender-specific words

-------------------------------------------------------------------------------------------------------------------------
"NOTEBOOK: EMOJIFY"
-------------------------------------------------------------------------------------------------------------------------
You are going to use word vector representations to build an Emojifier.
Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. So rather than writing "Congratulations on the promotion! Lets get coffee and talk. Love you!" the emojifier can automatically turn this into "Congratulations on the promotion! üëç Lets get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è"
You will implement a model which inputs a sentence (such as "Let's go see the baseball game tonight!") and finds the most appropriate emoji to be used with this sentence (‚öæ). In many emoji interfaces, you need to remember that ‚ù§ is the "heart" symbol rather than the "love" symbol. But using word vectors, you`ll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don`t even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set.
In this exercise, you`ll start with a baseline model (Emojifier-V1) using word embeddings, then build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM.


1 - Baseline model: Emojifier-V1
	1.1 - Dataset EMOJISET
Figure 1: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here.
	1.2 - Overview of the Emojifier-V1
Figure 2: Baseline model (Emojifier-V1).
	1.3 - Implementing Emojifier-V1




def sentence_to_avg(sentence, word_to_vec_map):
    # Step 1: Split sentence into list of lower case words (‚âà 1 line)
    words = sentence.lower().split()

    # Initialize the average word vector, should have the same shape as your word vectors.
    avg = np.zeros(word_to_vec_map[words[0]].shape)
    
    # Step 2: average the word vectors. You can loop over the words in the list "words".
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg/float(len(words))
    
    ### END CODE HERE ###
    
    return avg


z(i)=W.avg(i)+b
 
a(i)=softmax(z(i))
	 ny‚àí1
L(i)=‚àí‚àë Yoh(i)k ‚àó log(a(i)k)
	 k=0
	1.4 - Examining test set performance
2 - Emojifier-V2: Using LSTMs in Keras:
	2.1 - Overview of the model
	2.2 - Keras and mini-batching
	2.3 - The Embedding layer
	2.4 - Building the Emojifier-V2
-------------------------------------------------------------------------------------------------------------------------
Graded: Natural Language Processing & Word Embeddings
Graded: Operations on word vectors - Debiasing
Graded: Emojify
