************************************************************************************************************************************************************
SHALLOW NEURAL NETWORKS

Shallow Neural Network
	Neural Networks Overview
	Neural Network Representation
	Computing a Neural Network`s Output
	Vectorizing across multiple examples
	Explanation For Vectorized Implementation
	Activation functions
	Why do you need non-linear activation functions?
	Derivatives of activation functions
	Gradient descent For Neural Networks
	Backpropagation intuition 
	R&om Initialization

Practice Questions
	Quiz: Shallow Neural Networks

Programming Assignment
	Notebook: Planar data classIfication With a hidden layer
	@Programming Assignment: Planar data classIfication With a hidden layer

Heroes of Deep Learning 
	Ian Goodfellow interview
******************************************************************************************************************************************************
Shallow Neural Network
NEURAL NETWORKS OVERVIEW

---------------------------------------------------------------------------------------------------------------------------------------------
NEURAL NETWORK REPRESENTATION
A also st&s For activations, & it refers to the values that dIfferent layers of the neural network are passing on to the subsequent layers. 








---------------------------------------------------------------------------------------------------------------------------------------------
COMPUTING A NEURAL NETWORK`S OUTPUT









---------------------------------------------------------------------------------------------------------------------------------------------
VECTORIZING ACROSS MULTIPLE EXAMPLES









---------------------------------------------------------------------------------------------------------------------------------------------
EXPLANATION FOR VECTORIZED IMPLEMENTATION
---------------------------------------------------------------------------------------------------------------------------------------------
ACTIVATION FUNCTIONS
Sigmoid, tanh, Relu(RectIfied Linear Unit) , Leaky_RelU
	Sigmoid (z) = 1 / ( 1 + exp(-z)) - Use only For output layer
			when z is positive, Sigmoid(z) = 0.5 > Sigmoid(z) > 1
			when z is negative, Sigmoid(z) = 0 	 < Sigmoid(z) < 0.5
			when z is 0		  , Sigmoid(z) = 0.5


	tanh (z) = exp(z) - exp(-z) /  exp(z)  + exp(-z)
			when z is positive, tanh(z) = positive
			when z is negative, tanh(z) = negative
			when z is 0		  , tanh(z) = 0



	Relu (z) = max(0,z)
			when z is positive, Relu(z) = z
			when z is negative, Relu(z) = 0
			when z is 0		  , ReLU(z)	= 0.00000000000000


	Leaky_RelU() = max(0,z)
			when z is positive, Relu(z) = z
			when z is negative, Relu(z) = 0.01z
			when z is 0		  , ReLU(z)	= 0.00000000000000		

RULES OF THUMB FOR CHOOSING ACTIVATION FUNCTIONS.
	- If your output is 0, 1 value, If you`re using binary classIfication, then the sigmoid activation function is a very natural choice For the output layer. And then For all other unit`s ReLU, or the rectIfied linear unit, Is increasingly the default choice of activation function. 
	- So If you`re not sure what to use For your hidden layer, I would just use the "ReLU" activation function.

The advantage of both the ReLU & the leaky ReLU is that For a lot of the space of Z, the derivative of the activation function, the slope of the activation function is very dIfferent from 0.  And so in practice, using the ReLU activation function, your neural network will often learn much faster than when using the tanh or the sigmoid activation function. And the main reason is that there is less of these effects of the slope of the function going to 0, which slows down learning.  And I know that For half of the range of z, the slope of ReLU is 0, but in practice, enough of your hidden units will have z greater than 0. So learning can still be quite fast For most training examples. 


---------------------------------------------------------------------------------------------------------------------------------------------
WHY DO YOU NEED NON-LINEAR ACTIVATION FUNCTIONS?
---------------------------------------------------------------------------------------------------------------------------------------------
DERIVATIVES OF ACTIVATION FUNCTIONS
If activation function Is g(z) then its derivative can be denoted as " g`(z) "

g(z) |	sigmoid(z) 			|		        tanh(z)					|			max(0,z)		|  max(0.01z , z )		|
	 |						|										|							|						|
	 |	1 / (1 + exp(-z))	|exp(z) - exp(-z) /  exp(z)  + exp(-z)  |							|						|
-------------------------------------------------------------------------------------------------------------------------
g`(z)|	g(z)(1-g(z))		|			1 - g(z) ^ 2				|	1 If z >= 0 else 0		|1 If z>=0 else 0.01	|	





---------------------------------------------------------------------------------------------------------------------------------------------
GRADIENT DESCENT FOR NEURAL NETWORKS








---------------------------------------------------------------------------------------------------------------------------------------------
BACKPROPAGATION INTUITION 
---------------------------------------------------------------------------------------------------------------------------------------------
RANDOM INITIALIZATION
Symmetry breaking problem
Cannot initialize weights to 0

Logistic regressionâ€™s weights w should be initialized r&omly rather than to all zeros, because If you initialize to all zeros, then logistic regression will fail to learn a useful decision boundary because it will fail to â€œbreak symmetryâ€


No, Logistic Regression doesn`t have a hidden layer. If you initialize the weights to zeros, the first example x fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input x (because there`s no hidden layer) which is not zero. So at the second iteration, the weights values follow x`s distribution & are dIfferent From each other If x is not a constant vector.
---------------------------------------------------------------------------------------------------------------------------------------------

Practice Questions
	Quiz: Shallow Neural Networks

Programming Assignment
By completing this assignment you will:
- Develop an intuition of back-propagation & see it work on data.
- Recognize that the more hidden layers you have the more complex structure you could capture.
- Build all the helper functions to implement a full model With one hidden layer.

	Notebook: Planar data classIfication With a hidden layer
	@Programming Assignment: Planar data classIfication With a hidden layer
	- Implement a 2-class classIfication neural network With a single hidden layer
	- Use units With a non-linear activation function, such as tanh
	- Compute the cross entropy loss
	- Implement Forward & backward propagation

Interpretation: The dataset is not linearly separable, so logistic regression doesn`t perForm well. Hopefully a neural network will do better. 
4 - NEURAL NETWORK MODEL
The general methodology to build a Neural Network Is to:

1. Define the neural network structure (of input units,   of hidden units, etc). 
2. Initialize the model`s parameters
3. Loop:
    - Implement Forward propagation
    - Compute loss
    - Implement backward propagation to get the gradients
    - Update parameters (gradient descent)
You often build helper functions to compute steps 1-3 & then merge them into one function we call nn_model(). Once you`ve built nn_model() & learnt the right parameters, you can make predictions on new data.

-------------------------------------------------------------------
4.1 - DEFINING THE NEURAL NETWORK STRUCTURE
Exercise: Define three variables:

- n_x: the size of the input layer
- n_h: the size of the hidden layer (set this to 4) 
- n_y: the size of the output layer
Hint: Use shapes of X And Y to find n_x And n_y. Also, hard code the hidden layer size to be 4.

def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """
    ### START CODE HERE ### (â‰ˆ 3 lines of code)
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = X.shape[0] # size of output layer
    ### END CODE HERE ###
    return (n_x, n_h, n_y)
-------------------------------------------------------------------
4.2 - INITIALIZE THE MODEL`S PARAMETERS
Exercise: Implement the function initialize_parameters().
Instructions:
- Make sure your parameters` sizes are right. Refer to the neural network figure above if needed.
- You will initialize the weights matrices with random values.
	Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b).
- You will initialize the bias vectors as zeros.
	Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros.

def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """
    
    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.

    
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = np.random.randn(n_h,n_x) * 0.01 
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h) * 0.01 
    b2 = np.zeros((n_y,1))
    ### END CODE HERE ###
    
    assert (W1.shape == (n_h, n_x))
    assert (b1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (b2.shape == (n_y, 1))
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
>>>
W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[ 0.]
 [ 0.]
 [ 0.]
 [ 0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[ 0.]]
-------------------------------------------------------------------
4.3 - THE LOOP
Question: Implement forward_propagation().

Instructions:

- Look above at the mathematical representation of your classifier.
- You can use the function sigmoid(). It is built-in (imported) in the notebook.
- You can use the function np.tanh(). It is part of the numpy library.
- The steps you have to implement are:
	1. Retrieve each parameter from the dictionary "parameters" (which is the output of initialize_parameters()) by using parameters[".."].
	2. Implement Forward Propagation. Compute  Z[1],A[1],Z[2] and A[2] (the vector of all your predictions on all the examples in the training set).
Values needed in the backpropagation are stored in "cache". The cache will be given as an input to the backpropagation function.

def forward_propagation(X, parameters):
    """
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
    """
    # Retrieve each parameter from the dictionary "parameters"
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    ### END CODE HERE ###
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    Z1 = (W1 @ X) + b1
    A1 = np.tanh(Z1)
    Z2 = (W2 @ A1) + b2
    A2 = sigmoid(Z2)
    ### END CODE HERE ###
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache

X_assess, parameters = forward_propagation_test_case()
A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that your output matches ours. 
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
>>> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719z

-------------------------------------------------------------------
Now that you have computed  A[2]A[2]  (in the Python variable "A2"), which contains  a[2](i)a[2](i)  for every example, you can compute the cost function as follows:
		â‚˜
J=âˆ’1/m  âˆ‘	(  yâ½á¶¦â¾ log(a[Â²]â½á¶¦â¾)  +  (1âˆ’yâ½á¶¦â¾)  log(1âˆ’a[Â²]â½á¶¦â¾) )
	   á¶¦â¼Â¹
 
Exercise: Implement compute_cost() to compute the value of the cost  JJ .

Instructions:
																												 â‚˜
There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented  âˆ‘ yâ½á¶¦â¾ log(a[Â²]â½á¶¦â¾) :
																												á¶¦â¼Â¹
logprobs = np.multiply(np.log(A2),Y)
cost = - np.sum(logprobs)                # no need to use a for loop!



(you can use either np.multiply() and then np.sum() or directly np.dot()).
Note that if you use np.multiply followed by np.sum the end result will be a type float, whereas if you use np.dot, the result will be a 2D numpy array. We can use np.squeeze() to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array). We can cast the array as a type float using float().

def compute_cost(A2, Y, parameters):
    """
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    [Note that the parameters argument is not used in this function, 
    but the auto-grader currently expects this parameter.
    Future version of this notebook will fix both the notebook 
    and the auto-grader so that `parameters` is not needed.
    For now, please include `parameters` in the function signature,
    and also when invoking this function.]
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    
    """
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),(1-Y))
    cost = - (np.sum(logprobs))/m
    ### END CODE HERE ###
    
    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. 
                                    # E.g., turns [[17]] into 17 
    assert(isinstance(cost, float))
    
    return cost

A2, Y_assess, parameters = compute_cost_test_case()
print("cost = " + str(compute_cost(A2, Y_assess, parameters)))
>>>cost = 0.6930587610394646


-----------------------------------------------------
Using the cache computed during forward propagation, you can now implement backward propagation.

Question: Implement the function backward_propagation().

Instructions: Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You`ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.
"https://raw.githubusercontent.com/Gurubux/DL_Coursera/master/1_Neural%20Networks%20and%20Deep%20Learning/1_3_Shallow_Neural_Networks/back_propogation.png"
Tips:
	To compute dZ1 you`ll need to compute  g[1]â€²(Z[1]) .
	Since  g[1](.) is the tanh activation function, if  a=g[1](z) then  g[1]â€²(z) = 1âˆ’a2. 
	So you can compute  g[1]â€²(Z[1])  using (1 - np.power(A1, 2)).


# GRADED FUNCTION: backward_propagation

def backward_propagation(parameters, cache, X, Y):
    """
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
    X -- input data of shape (2, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary "parameters".
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    W1 = parameters['W1']
    W2 = parameters['W2']
    ### END CODE HERE ###
        
    # Retrieve also A1 and A2 from dictionary "cache".
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A1 = cache['A1']
    A2 = cache['A2']
    ### END CODE HERE ###
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    ### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
    dZ2 = A2 - Y
    dW2 = (dZ2 @ A1.T)/m
    db2 = np.sum(dZ2,axis = 1, keepdims=True)/m
    dZ1 = np.dot(W2.T ,dZ2) * (1 - np.power(A1, 2))
    dW1 = (dZ1 @ X.T)/m
    db1 = np.sum(dZ1,axis = 1, keepdims=True)/m
    ### END CODE HERE ###
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads

    parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print ("dW1 = "+ str(grads["dW1"]))
print ("db1 = "+ str(grads["db1"]))
print ("dW2 = "+ str(grads["dW2"]))
print ("db2 = "+ str(grads["db2"]))
>>>
dW1 = [[ 0.00301023 -0.00747267]
[ 0.00257968 -0.00641288]
[-0.00156892  0.003893  ]
[-0.00652037  0.01618243]]
db1 = [[ 0.00176201]
[ 0.00150995]
[-0.00091736]
[-0.00381422]]
dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]
db2 = [[-0.16655712]]
-----------------------------------------------------------------------------------------
Question: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).
General gradient descent rule:  Î¸ = Î¸ âˆ’ Î± (âˆ‚/âˆ‚Î¸)J   where  Î± is the learning rate and  Î¸represents a parameter.
Illustration: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.
Good SGD - "https://raw.githubusercontent.com/Gurubux/DL_Coursera/master/1_Neural%20Networks%20and%20Deep%20Learning/1_3_Shallow_Neural_Networks/sgd.gif"
Bad SGD - "https://raw.githubusercontent.com/Gurubux/DL_Coursera/master/1_Neural%20Networks%20and%20Deep%20Learning/1_3_Shallow_Neural_Networks/sgd_bad.gif"


# GRADED FUNCTION: update_parameters

def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    """
    # Retrieve each parameter from the dictionary "parameters"
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    ### END CODE HERE ###
    
    # Retrieve each gradient from the dictionary "grads"
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']
    ## END CODE HERE ###
    
    # Update rule for each parameter
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = W1 - learning_rate * (dW1)
    b1 = b1 - learning_rate * (db1)
    W2 = W2 - learning_rate * (dW2)
    b2 = b2 - learning_rate * (db2)
    ### END CODE HERE ###
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))



--------------------------------------------------------------------------------------
4.4 - INTEGRATE PARTS 4.1, 4.2 AND 4.3 IN NN_MODEL()
Question: Build your neural network model in nn_model().

Instructions: The neural network model has to use the previous functions in the right order.

# GRADED FUNCTION: nn_model

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters
    ### START CODE HERE ### (â‰ˆ 1 line of code)
    parameters = initialize_parameters(n_x, n_h, n_y)
    ### END CODE HERE ###
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        ### START CODE HERE ### (â‰ˆ 4 lines of code)
        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
        parameters = update_parameters(parameters, grads)
        
        ### END CODE HERE ###
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters

X_assess, Y_assess = nn_model_test_case()
parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))

>>>
Cost after iteration 0: 0.692739
Cost after iteration 1000: 0.000218
Cost after iteration 2000: 0.000107
Cost after iteration 3000: 0.000071
Cost after iteration 4000: 0.000053
Cost after iteration 5000: 0.000042
Cost after iteration 6000: 0.000035
Cost after iteration 7000: 0.000030
Cost after iteration 8000: 0.000026
Cost after iteration 9000: 0.000023
W1 = [[-0.65848169  1.21866811]
 [-0.76204273  1.39377573]
 [ 0.5792005  -1.10397703]
 [ 0.76773391 -1.41477129]]
b1 = [[ 0.287592  ]
 [ 0.3511264 ]
 [-0.2431246 ]
 [-0.35772805]]
W2 = [[-2.45566237 -3.27042274  2.00784958  3.36773273]]
b2 = [[ 0.20459656]]


----------------------------------------------------------------------------------
4.5 PREDICTIONS
Question: Use your model to predict by building predict(). Use forward propagation to predict results.
Reminder: predictions =  yprediction=ðŸ™{activation > 0.5}={10if activation>0.5otherwiseyprediction=1{activation > 0.5}={1if activation>0.50otherwise 
As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X > threshold)

---------------------------------------------------------------------------------------------------------------------------------------------
Heroes of Deep Learning 
	Ian Goodfellow interview

