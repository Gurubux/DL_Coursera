************************************************************************************************************************************************************
NEURAL NETWORKS BASICS
Vectorization
-----------------------------------------------------------------------------------------------------------------------------------------------------
Logistic Regression as a Neural Network
	Binary Classification
	Logistic Regression
	Logistic Regression Cost Function
	Gradient Descent
	Derivatives
	More Derivative Examples
		 - Derivative just means slope of a line.
		 - The slope of a function can be different at different points on the function. 
		 	In our first example where f(a) = 3a those a straight line. The derivative was the "same" everywhere, it was three everywhere. 
		 	For other functions like   f(a) = a² or f(a) = log(a), the slope of the line "varies". So, the slope or the derivative can be different at different points on the curve.

	Computation graph
		The computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives. The computation graph explains why it is organized this way.
		Through a "left-to-right" pass, you can compute the value of J and what we`ll see in the next couple of slides is that in order to compute derivatives there`ll be a right-to-left pass like this, kind of going in the opposite direction as the blue arrows. That would be most natural for computing the derivatives. So to recap, the computation graph organizes a computation with this blue arrow, left-to-right computation. Let's refer to the next video how you can do the backward red arrow right-to-left computation of the derivatives. 
	Derivatives with a Computation Graph
		In this class, what does the coding convention "dvar" represent?
			The derivative of a final output variable with respect to various intermediate quantities.

			
	Logistic Regression Gradient Descent
	Gradient Descent on m Examples
-----------------------------------------------------------------------------------------------------------------------------------------------------
Python and Vectorization
	Vectorization
	More Vectorization Examples
	Vectorizing Logistic Regression
	Vectorizing Logistic Regression`s Gradient Output
	Broadcasting in Python
	A note on python/numpy vectors
	Quick tour of Jupyter/iPython Notebooks
	Explanation of logistic regression cost function 
		Minimizing the loss corresponds with maximizing logp(y|x).
-----------------------------------------------------------------------------------------------------------------------------------------------------
Practice Questions
	Quiz: Neural Network Basics
-----------------------------------------------------------------------------------------------------------------------------------------------------
Programming Assignments
	Reading: ReadingDeep Learning Honor Code
	Reading: ReadingProgramming Assignment FAQ
	Notebook: Python Basics with numpy 
		Normalization: 
			It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to x/∥x∥(dividing each row vector of x by its norm).

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
- np.exp(x) works for any np.array x and applies the exponential function to every coordinate
- the sigmoid function and its gradient
- image2vector is commonly used in deep learning
- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.
- numpy has efficient built-in functions
- broadcasting is extremely useful
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


		Note that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to  .* in Matlab/Octave), which performs an element-wise multiplication.
\################# CLASSIC DOT PRODUCT ##################
x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###
tic = time.process_time()
dot = 0
for i in range(len(x1)):
    dot+= x1[i]*x2[i]
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC OUTER PRODUCT IMPLEMENTATION ###
tic = time.process_time()
outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros
for i in range(len(x1)):
    for j in range(len(x2)):
        outer[i,j] = x1[i]*x2[j]
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC ELEMENTWISE IMPLEMENTATION ###
tic = time.process_time()
mul = np.zeros(len(x1))
for i in range(len(x1)):
    mul[i] = x1[i]*x2[i]
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###
W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array
tic = time.process_time()
gdot = np.zeros(W.shape[0])
for i in range(W.shape[0]):
    for j in range(len(x1)):
        gdot[i] += W[i,j]*x1[j]
toc = time.process_time()
print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")


\################# VECTORIZED DOT PRODUCT ##################

x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### VECTORIZED DOT PRODUCT OF VECTORS ###
tic = time.process_time()
dot = np.dot(x1,x2)
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED OUTER PRODUCT ###
tic = time.process_time()
outer = np.outer(x1,x2)
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED ELEMENTWISE MULTIPLICATION ###
tic = time.process_time()
mul = np.multiply(x1,x2)
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED GENERAL DOT PRODUCT ###
tic = time.process_time()
dot = np.dot(W,x1)
toc = time.process_time()
print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")








@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
- Vectorization is very important in deep learning. It provides computational efficiency and clarity.
- You have reviewed the L1 and L2 loss.
- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
----------------------------------------------------------------------------------------------------------------------------------------------------


	PRACTICE PROGRAMMING ASSIGNMENT: PYTHON BASICS WITH NUMPY 
	Notebook: Logistic Regression with a Neural Network mindset
	@Programming Assignment: Logistic Regression with a Neural Network mindset


2. 	Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px  * num_px * 3, 1).
	A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b * c * d, a) is to use:
		X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X



2. One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Common steps for pre-processing a new dataset are:
	- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
	- Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)
	- "Standardize" the data
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

\Logistic Regression is actually a very simple Neural Network!
"4 - BUILDING THE PARTS OF OUR ALGORITHM"

The main steps For building a Neural Network are:

	1. Define the model structure (such as number of input features)
	2. Initialize the model`s parameters
	3. Loop:
		Calculate current loss (forward propagation)
		Calculate current gradient (backward propagation)
		Update parameters (gradient descent)
	You often build 1-3 separately and integrate them into one function we call model().

	4.1 - Helper functions
	4.2 - Initializing parameters
	4.3 - Forward and Backward propagation
	4.4 - Optimization
			You have initialized your parameters.
			You are also able to compute a cost function and its gradient.
			Now, you want to update the parameters using gradient descent.
				θ = θ − α(dθ) 



There are two steps to computing predictions:
	Calculate  Ŷ =A=σ(wTX+b)Y^=A=σ(wTX+b) 
	Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this).


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
You`ve implemented several functions that:
- Initialize (w,b)
- Optimize the loss iteratively to learn parameters (w,b):
	 - computing the cost and its gradient
	 - updating the parameters using gradient descent
- Use the learned (w,b) to predict the labels for a given set of examples
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


"5 - MERGE ALL FUNCTIONS INTO A MODEL"

-----------------------------------------------------------------------------------------------------------------------------------------------------
Heroes of Deep Learning -
	Pieter Abbeel interview